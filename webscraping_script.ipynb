{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"                      # enter your keys\n",
    "os.environ[\"SERPER_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_html_from_url(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        excluded_tagNames = [\"footer\", \"nav\"]\n",
    "        for tag_name in excluded_tagNames:\n",
    "            for unwanted_tag in soup.find_all(tag_name):\n",
    "                unwanted_tag.extract()\n",
    "        for a_tag in soup.find_all(\"a\"):\n",
    "            href = a_tag.get(\"href\")\n",
    "            if href:\n",
    "                a_tag.string = f\"{a_tag.get_text()} ({href})\"\n",
    "        return ' '.join(soup.stripped_strings) \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sites_generator(agent_input:str, no_websites:int):\n",
    "    \"enter the search query and number of websites ,it will return url of related sites\"\n",
    "    serper_search = GoogleSerperAPIWrapper(k=no_websites)\n",
    "    websites = serper_search.results(agent_input)\n",
    "    campaigns = websites['organic']\n",
    "    return campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example : Change according to content stucture you want\n",
    "\n",
    "class CampaignItem(BaseModel):\n",
    "    campaign_name: str = Field(description=\"The name of sports campaign\")\n",
    "    description: str = Field(description=\"Detail description of the campaign \")\n",
    "\n",
    "class CampaignList(BaseModel):\n",
    "    campaigns: List[CampaignItem]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "output_parser = PydanticOutputParser(pydantic_object = CampaignList)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an expert making web scrapping and analyzing HTML raw code.\n",
    "If there is no explicit information don't make any assumption.\n",
    "Extract all objects that matched the instructions from the following html\n",
    "{html_text}\n",
    "Provide them in a list, also if there is a next page link remember to add it to the object.\n",
    "Please, follow carefulling the following instructions\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"html_text\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    # use token count for more accurate results\n",
    "    return len(text)\n",
    "\n",
    "def extract_content(campaigns):\n",
    "    for camp in campaigns:\n",
    "        url = camp['link']\n",
    "        html_text_parsed = extract_html_from_url(url)\n",
    "        \n",
    "        token_count = count_tokens(html_text_parsed)\n",
    "        \n",
    "        if token_count < 15000:\n",
    "            response = chain.invoke(input={\"html_text\": html_text_parsed})\n",
    "            campaigns_extracted = response.campaigns\n",
    "        else:\n",
    "            # Use Langchain Recursive Splitter to split the text\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=15000, chunk_overlap=500)\n",
    "            splits = splitter.split_text(html_text_parsed)\n",
    "            campaigns_extracted = []\n",
    "            for split in splits:\n",
    "                response = chain.invoke(input={\"html_text\": split})\n",
    "                campaigns_extracted.extend(response.campaigns)\n",
    "\n",
    "        rows = []\n",
    "        columns = ['campaign_name', 'description']\n",
    "        for campaign_extracted in campaigns_extracted:\n",
    "            data = {\n",
    "                \"campaign_name\": campaign_extracted.campaign_name,\n",
    "                \"description\": campaign_extracted.description\n",
    "            }\n",
    "            rows.append(data)\n",
    "\n",
    "        result_df = pd.DataFrame(rows, columns=columns)\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_input = \"\"\"Scrape all the football campaign\"\"\"\n",
    "campaigns = sites_generator(agent_input=agent_input,no_websites=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = extract_content(campaigns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmscrap",
   "language": "python",
   "name": "llmscrap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
